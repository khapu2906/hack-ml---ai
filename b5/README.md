### âœ… **Day 5 â€“ Feature Selection & Tuning mÃ´ hÃ¬nh ML**

**ğŸ¯ Má»¥c tiÃªu:** Biáº¿t cÃ¡ch chá»n lá»±a cÃ¡c Ä‘áº·c trÆ°ng (features) quan trá»ng, tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh vá»›i Grid Search vÃ  Cross-validation Ä‘á»ƒ cáº£i thiá»‡n káº¿t quáº£.

---

#### 1. Feature Importance & Feature Selection

##### ğŸ“Œ Vá»›i mÃ´ hÃ¬nh cÃ¢y (Tree-based models):

```python
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_
feature_names = X_train.columns

feat_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print(feat_df)
```

**Giáº£i thÃ­ch:**

*   `RandomForestClassifier`: MÃ´ hÃ¬nh Random Forest.
*   `feature_importances_`: Thuá»™c tÃ­nh tráº£ vá» Ä‘á»™ quan trá»ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng.
*   `feat_df`: DataFrame chá»©a tÃªn Ä‘áº·c trÆ°ng vÃ  Ä‘á»™ quan trá»ng cá»§a chÃºng, Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± giáº£m dáº§n cá»§a Ä‘á»™ quan trá»ng.

##### ğŸ“Œ Lá»c theo thá»‘ng kÃª (SelectKBest):

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X_train, y_train)

selected_features = X_train.columns[selector.get_support()]
print("Top features:", list(selected_features))
```

**Giáº£i thÃ­ch:**

*   `SelectKBest`: Lá»›p Ä‘á»ƒ chá»n cÃ¡c Ä‘áº·c trÆ°ng tá»‘t nháº¥t dá»±a trÃªn má»™t hÃ m Ä‘Ã¡nh giÃ¡.
*   `f_classif`: HÃ m Ä‘Ã¡nh giÃ¡ sá»­ dá»¥ng phÃ¢n tÃ­ch phÆ°Æ¡ng sai (ANOVA) Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm cho cÃ¡c Ä‘áº·c trÆ°ng.
*   `k`: Sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng tá»‘t nháº¥t cáº§n chá»n.
*   `get_support()`: PhÆ°Æ¡ng thá»©c tráº£ vá» má»™t máº£ng boolean cho biáº¿t cÃ¡c Ä‘áº·c trÆ°ng nÃ o Ä‘Ã£ Ä‘Æ°á»£c chá»n.

**ğŸ“š Tham kháº£o:**

*   [Scikit-learn documentation - RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
*   [Scikit-learn documentation - SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)

---

#### 2. Tá»‘i Æ°u mÃ´ hÃ¬nh báº±ng Grid Search

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    "n_estimators": [50, 100],
    "max_depth": [None, 5, 10],
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring="accuracy")
grid.fit(X_train, y_train)

print("Best Params:", grid.best_params_)
print("Best Score:", grid.best_score_)
```

**Giáº£i thÃ­ch:**

*   `GridSearchCV`: Lá»›p Ä‘á»ƒ tÃ¬m kiáº¿m cÃ¡c tham sá»‘ tá»‘t nháº¥t cho mÃ´ hÃ¬nh báº±ng cÃ¡ch thá»­ táº¥t cáº£ cÃ¡c tá»• há»£p tham sá»‘ cÃ³ thá»ƒ.
*   `param_grid`: Tá»« Ä‘iá»ƒn chá»©a cÃ¡c tham sá»‘ vÃ  cÃ¡c giÃ¡ trá»‹ cáº§n thá»­.
*   `cv`: Sá»‘ lÆ°á»£ng fold trong cross-validation.
*   `scoring`: HÃ m Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.
*   `best_params_`: Thuá»™c tÃ­nh tráº£ vá» cÃ¡c tham sá»‘ tá»‘t nháº¥t.
*   `best_score_`: Thuá»™c tÃ­nh tráº£ vá» Ä‘iá»ƒm sá»‘ tá»‘t nháº¥t.

**ğŸ“š Tham kháº£o:**

*   [Scikit-learn documentation - GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

---

#### 3. Cross-validation (CV)

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring="accuracy")
print("CV Accuracy Scores:", scores)
print("Mean Accuracy:", scores.mean())
```

**Giáº£i thÃ­ch:**

*   `cross_val_score`: HÃ m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh báº±ng cross-validation.
*   `cv`: Sá»‘ lÆ°á»£ng fold trong cross-validation.
*   `scoring`: HÃ m Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.

**ğŸ“š Tham kháº£o:**

*   [Scikit-learn documentation - cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

---

#### 4. So sÃ¡nh nhiá»u mÃ´ hÃ¬nh vá»›i Pipeline + GridSearch

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression())
])

param_grid = {
    "clf__C": [0.1, 1, 10],
    "clf__penalty": ["l2"],
}

grid = GridSearchCV(pipeline, param_grid, cv=5)
grid.fit(X_train, y_train)

print("Best Params:", grid.best_params_)
```

**Giáº£i thÃ­ch:**

*   `Pipeline`: Cho phÃ©p báº¡n káº¿t há»£p nhiá»u bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh thÃ nh má»™t quy trÃ¬nh duy nháº¥t.
*   `StandardScaler`: Chuáº©n hÃ³a dá»¯ liá»‡u báº±ng cÃ¡ch loáº¡i bá» giÃ¡ trá»‹ trung bÃ¬nh vÃ  chia cho Ä‘á»™ lá»‡ch chuáº©n.
*   `clf__C`: Tham sá»‘ C cá»§a mÃ´ hÃ¬nh Logistic Regression.
*   `clf__penalty`: Tham sá»‘ penalty cá»§a mÃ´ hÃ¬nh Logistic Regression.

**ğŸ“š Tham kháº£o:**

*   [Scikit-learn documentation - Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
*   [Scikit-learn documentation - StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)

---

### ğŸ§ª BÃ i Lab Day 5

1.  Ãp dá»¥ng `SelectKBest` Ä‘á»ƒ chá»n 5 features tá»‘t nháº¥t
2.  DÃ¹ng RandomForest Ä‘á»ƒ hiá»ƒn thá»‹ feature importance
3.  DÃ¹ng `GridSearchCV` tá»‘i Æ°u Logistic vÃ  RandomForest
4.  Ghi káº¿t quáº£ `best_params_` vÃ  `best_score_` vÃ o file `model_result.json`

**HÆ°á»›ng dáº«n:**

*   Sá»­ dá»¥ng `SelectKBest` Ä‘á»ƒ chá»n 5 Ä‘áº·c trÆ°ng tá»‘t nháº¥t tá»« táº­p dá»¯ liá»‡u Titanic.
*   Sá»­ dá»¥ng `RandomForestClassifier` Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  hiá»ƒn thá»‹ Ä‘á»™ quan trá»ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng.
*   Sá»­ dá»¥ng `GridSearchCV` Ä‘á»ƒ tÃ¬m cÃ¡c tham sá»‘ tá»‘t nháº¥t cho mÃ´ hÃ¬nh `LogisticRegression` vÃ  `RandomForestClassifier`.
*   LÆ°u cÃ¡c tham sá»‘ tá»‘t nháº¥t vÃ  Ä‘iá»ƒm sá»‘ vÃ o file `model_result.json`.

---

### ğŸ“ BÃ i táº­p vá» nhÃ  Day 5:

1.  So sÃ¡nh mÃ´ hÃ¬nh Logistic Regression, Decision Tree vÃ  Random Forest sau khi tuning
2.  Táº¡o file `compare_models.py` Ä‘á»ƒ hiá»ƒn thá»‹ báº£ng so sÃ¡nh accuracy, precision, recall
3.  LÆ°u cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t vÃ o `models/`
4.  Ghi nháº­t kÃ½ tuning trong `notebooks/05_model_tuning.ipynb`
5.  Commit vÃ  Ä‘áº©y lÃªn GitHub

**HÆ°á»›ng dáº«n:**

*   Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh `LogisticRegression`, `DecisionTreeClassifier`, vÃ  `RandomForestClassifier` Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng `GridSearchCV`.
*   Táº¡o má»™t báº£ng so sÃ¡nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh (accuracy, precision, recall, f1-score).
*   LÆ°u cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t vÃ o thÆ° má»¥c `models/`.
*   Ghi láº¡i quÃ¡ trÃ¬nh tuning vÃ  káº¿t quáº£ vÃ o notebook `notebooks/05_model_tuning.ipynb`.
