## üìö Bu·ªïi 8 ‚Äì Nh·∫≠p m√¥n RNN v√† NLP

### üéØ M·ª•c ti√™u:

*   Hi·ªÉu kh√°i ni·ªám v·ªÅ d·ªØ li·ªáu tu·∫ßn t·ª± v√† ·ª©ng d·ª•ng trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP)
*   N·∫Øm v·ªØng ki·∫øn tr√∫c RNN, LSTM, GRU
*   Bi·∫øt c√°ch x·ª≠ l√Ω vƒÉn b·∫£n v√† x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n c∆° b·∫£n b·∫±ng RNN

---

### üîç N·ªôi dung ch√≠nh:

#### 1. **D·ªØ li·ªáu tu·∫ßn t·ª± l√† g√¨?**

*   **D·ªØ li·ªáu tu·∫ßn t·ª±** l√† lo·∫°i d·ªØ li·ªáu c√≥ th·ª© t·ª± c√°c y·∫øu t·ªë (chu·ªói), th√¥ng tin c·ªßa m·ªôt y·∫øu t·ªë ph·ª• thu·ªôc v√†o y·∫øu t·ªë tr∆∞·ªõc ƒë√≥.
    *   V√≠ d·ª•: VƒÉn b·∫£n, √¢m thanh, chu·ªói th·ªùi gian (time series).
*   **·ª®ng d·ª•ng**: Ph√¢n t√≠ch c·∫£m x√∫c, chatbot, d·ªãch m√°y, nh·∫≠n di·ªán gi·ªçng n√≥i, nh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay...
*   **Ph√¢n bi·ªát d·ªØ li·ªáu tu·∫ßn t·ª± v√† d·ªØ li·ªáu d·∫°ng b·∫£ng**:
    *   D·ªØ li·ªáu tu·∫ßn t·ª±: Th·ª© t·ª± c√°c y·∫øu t·ªë c√≥ √Ω nghƒ©a (v√≠ d·ª•: vƒÉn b·∫£n, √¢m thanh).
    *   D·ªØ li·ªáu d·∫°ng b·∫£ng: M·ªói d√≤ng kh√¥ng c√≥ s·ª± li√™n k·∫øt ch·∫∑t ch·∫Ω v·ªõi c√°c d√≤ng kh√°c (v√≠ d·ª•: d·ªØ li·ªáu tabular trong b·∫£ng t√≠nh).

**Gi·∫£i th√≠ch:**

*   **D·ªØ li·ªáu tu·∫ßn t·ª±:** D·ªØ li·ªáu m√† th·ª© t·ª± c·ªßa c√°c ph·∫ßn t·ª≠ c√≥ √Ω nghƒ©a quan tr·ªçng.
*   **·ª®ng d·ª•ng:** C√°c ·ª©ng d·ª•ng ph·ªï bi·∫øn c·ªßa d·ªØ li·ªáu tu·∫ßn t·ª±.
*   **Ph√¢n bi·ªát:** S·ª± kh√°c bi·ªát gi·ªØa d·ªØ li·ªáu tu·∫ßn t·ª± v√† d·ªØ li·ªáu d·∫°ng b·∫£ng.

#### 2. **Ki·∫øn tr√∫c RNN (Recurrent Neural Network)**:

*   **RNN** l√† lo·∫°i m·∫°ng n∆°-ron c√≥ kh·∫£ nƒÉng l∆∞u tr·ªØ th√¥ng tin qua c√°c b∆∞·ªõc th·ªùi gian (timesteps). M·ªói b∆∞·ªõc nh·∫≠n ƒë·∫ßu v√†o v√† th√¥ng tin tr·∫°ng th√°i (hidden state) t·ª´ b∆∞·ªõc tr∆∞·ªõc.
*   **C√¥ng th·ª©c c∆° b·∫£n**: $h_t = f(x_t, h_{t-1})$
    *   $x_t$ l√† ƒë·∫ßu v√†o t·∫°i th·ªùi ƒëi·ªÉm $t$
    *   $h_t$ l√† tr·∫°ng th√°i ·∫©n (hidden state) t·∫°i th·ªùi ƒëi·ªÉm $t$
*   **Vanishing Gradient Problem**: RNN g·∫∑p kh√≥ khƒÉn khi h·ªçc d·ªØ li·ªáu chu·ªói d√†i v√¨ gradient c√≥ th·ªÉ m·∫•t d·∫ßn trong qu√° tr√¨nh lan truy·ªÅn ng∆∞·ª£c (backpropagation).

**Gi·∫£i th√≠ch:**

*   **RNN:** M·∫°ng n∆°-ron c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu tu·∫ßn t·ª±.
*   **C√¥ng th·ª©c c∆° b·∫£n:** C√¥ng th·ª©c t√≠nh to√°n tr·∫°ng th√°i ·∫©n c·ªßa RNN.
*   **Vanishing Gradient Problem:** V·∫•n ƒë·ªÅ khi·∫øn RNN kh√≥ h·ªçc d·ªØ li·ªáu chu·ªói d√†i.

#### 3. **Gi·∫£i ph√°p: LSTM (Long Short-Term Memory) v√† GRU (Gated Recurrent Unit)**:

*   **LSTM**: Ki·∫øn tr√∫c ƒë·∫∑c bi·ªát v·ªõi c√°c c·ªïng (gates) ƒë·ªÉ ƒëi·ªÅu khi·ªÉn d√≤ng d·ªØ li·ªáu qua c√°c b∆∞·ªõc th·ªùi gian. LSTM c√≥ kh·∫£ nƒÉng ghi nh·ªõ th√¥ng tin d√†i h·∫°n.
*   **GRU**: L√† bi·∫øn th·ªÉ nh·∫π h∆°n c·ªßa LSTM, s·ª≠ d·ª•ng √≠t c·ªïng h∆°n nh∆∞ng v·∫´n hi·ªáu qu·∫£ trong vi·ªác l∆∞u tr·ªØ th√¥ng tin.
*   **So s√°nh LSTM v√† GRU**: LSTM c√≥ 3 c·ªïng (input, output, forget), trong khi GRU c√≥ 2 c·ªïng (reset, update). GRU nhanh v√† d·ªÖ hu·∫•n luy·ªán h∆°n, nh∆∞ng LSTM c√≥ kh·∫£ nƒÉng bi·ªÉu di·ªÖn m·ªëi quan h·ªá ph·ª©c t·∫°p h∆°n.

**Gi·∫£i th√≠ch:**

*   **LSTM:** Ki·∫øn tr√∫c RNN c·∫£i ti·∫øn ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ vanishing gradient.
*   **GRU:** M·ªôt bi·∫øn th·ªÉ ƒë∆°n gi·∫£n h∆°n c·ªßa LSTM.
*   **So s√°nh:** So s√°nh ∆∞u v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa LSTM v√† GRU.

#### 4. **Tokenization & Embedding trong NLP**:

*   **Tokenization**: Qu√° tr√¨nh chuy·ªÉn vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã nh·ªè h∆°n (tokens) nh∆∞ t·ª´ ho·∫∑c c√¢u. Tokenizer gi√∫p ph√¢n t√≠ch vƒÉn b·∫£n th√†nh c√°c token c√≥ th·ªÉ x·ª≠ l√Ω b·ªüi m√¥ h√¨nh.
*   **Embedding**: Chuy·ªÉn ƒë·ªïi c√°c token (word) th√†nh c√°c vector li√™n t·ª•c ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá gi·ªØa c√°c t·ª´.
    *   **One-hot encoding**: M·ªói t·ª´ l√† m·ªôt vector v·ªõi m·ªôt gi√° tr·ªã "1" t·∫°i v·ªã tr√≠ c·ªßa t·ª´ trong t·ª´ ƒëi·ªÉn, c√≤n l·∫°i l√† "0". C√°ch n√†y ƒë∆°n gi·∫£n nh∆∞ng r·∫•t th∆∞a (sparse).
    *   **Word2Vec**: D√πng m·∫°ng n∆°-ron ƒë·ªÉ h·ªçc c√°c vector t·ª´ trong kh√¥ng gian t·ª´ v·ª±ng d·ª±a tr√™n ng·ªØ c·∫£nh xung quanh.
    *   **GloVe**: T·∫°o embedding t·ª´ ph√¢n t√≠ch th·ªëng k√™ c√°c m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ trong m·ªôt corpus l·ªõn.

**Gi·∫£i th√≠ch:**

*   **Tokenization:** Qu√° tr√¨nh chia vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã nh·ªè h∆°n.
*   **Embedding:** Qu√° tr√¨nh chuy·ªÉn ƒë·ªïi c√°c token th√†nh vector.
*   **One-hot encoding:** M·ªôt ph∆∞∆°ng ph√°p embedding ƒë∆°n gi·∫£n.
*   **Word2Vec:** M·ªôt ph∆∞∆°ng ph√°p embedding s·ª≠ d·ª•ng m·∫°ng n∆°-ron.
*   **GloVe:** M·ªôt ph∆∞∆°ng ph√°p embedding d·ª±a tr√™n th·ªëng k√™.

#### 5. **Th·ª≠ nghi·ªám m√¥ h√¨nh ƒë∆°n gi·∫£n: Next Word Prediction**

*   M√¥ h√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo trong c√¢u.
*   Input: "T√¥i th√≠ch" ‚Üí Output: "h·ªçc"
*   C√°ch th·ª±c hi·ªán: S·ª≠ d·ª•ng RNN, token h√≥a vƒÉn b·∫£n v√† hu·∫•n luy·ªán m√¥ h√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo.

**Gi·∫£i th√≠ch:**

*   **Next Word Prediction:** M·ªôt b√†i to√°n NLP c∆° b·∫£n.
*   **C√°ch th·ª±c hi·ªán:** M√¥ t·∫£ c√°c b∆∞·ªõc ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo.

---

### üß™ B√†i lab Bu·ªïi 8:

#### 1. **Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n:**

*   S·ª≠ d·ª•ng `nltk` ƒë·ªÉ lo·∫°i b·ªè d·∫•u c√¢u, chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng (lowercase), v√† lo·∫°i b·ªè stopwords.

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def clean_text(text):
    text = text.lower()
    tokens = nltk.word_tokenize(re.sub(r'\W+', ' ', text))
    tokens = [w for w in tokens if w not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w) for w in tokens]

print(clean_text("I am learning Natural Language Processing in Python!"))
```

**H∆∞·ªõng d·∫´n:**

*   C√†i ƒë·∫∑t th∆∞ vi·ªán `nltk`.
*   T·∫£i c√°c t√†i nguy√™n c·∫ßn thi·∫øt c·ªßa `nltk`.
*   Vi·∫øt h√†m `clean_text()` ƒë·ªÉ th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n.

#### 2. **T·∫°o embedding v·ªõi PyTorch:**

*   T·∫°o embedding cho m·ªôt c√¢u s·ª≠ d·ª•ng PyTorch.

```python
import torch
import torch.nn as nn

vocab_size = 1000
embedding_dim = 50
embedding = nn.Embedding(vocab_size, embedding_dim)

sample_input = torch.tensor([1, 45, 234])
embedded = embedding(sample_input)
print(embedded.shape)  # (3, 50)
```

**H∆∞·ªõng d·∫´n:**

*   S·ª≠ d·ª•ng l·ªõp `nn.Embedding` c·ªßa PyTorch ƒë·ªÉ t·∫°o embedding.
*   T·∫°o m·ªôt tensor ƒë·∫ßu v√†o v√† chuy·ªÉn n√≥ qua l·ªõp embedding.

#### 3. **X√¢y d·ª±ng m√¥ h√¨nh RNN:**

*   M√¥ h√¨nh RNN ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo.

```python
class RNNModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        out, _ = self.rnn(x)
        return self.fc(out)

# Dummy input: batch_size=2, sequence_length=4
x = torch.randint(0, vocab_size, (2, 4))
model = RNNModel(vocab_size=1000, embed_size=64, hidden_size=128)
output = model(x)
print(output.shape)  # (2, 4, 1000)
```

**H∆∞·ªõng d·∫´n:**

*   T·∫°o m·ªôt l·ªõp `RNNModel` k·∫ø th·ª´a t·ª´ `nn.Module`.
*   S·ª≠ d·ª•ng l·ªõp `nn.RNN` c·ªßa PyTorch ƒë·ªÉ t·∫°o m√¥ h√¨nh RNN.
*   S·ª≠ d·ª•ng l·ªõp `nn.Linear` ƒë·ªÉ t·∫°o l·ªõp fully connected.

---

### üìù B√†i t·∫≠p v·ªÅ nh√† Bu·ªïi 8:

1.  **Vi·∫øt h√†m chu·∫©n h√≥a vƒÉn b·∫£n**:

    *   Vi·∫øt m·ªôt h√†m chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát: lo·∫°i b·ªè d·∫•u c√¢u, chuy·ªÉn v·ªÅ lowercase, lo·∫°i b·ªè stopwords (c√≥ th·ªÉ t·ª± ƒë·ªãnh nghƒ©a stopword list ti·∫øng Vi·ªát).
2.  **So s√°nh k·∫øt qu·∫£ embedding gi·ªØa One-Hot v√† Word2Vec**:

    *   D√πng `gensim` ƒë·ªÉ load word2vec ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh (GoogleNews).
    *   So s√°nh c√°c vector embedding gi·ªØa One-Hot v√† Word2Vec.
3.  **Vi·∫øt m√¥ h√¨nh GRU ƒë·ªÉ sinh vƒÉn b·∫£n**:

    *   Cho m·ªôt ƒëo·∫°n vƒÉn m·∫´u, vi·∫øt m√¥ h√¨nh GRU ƒë∆°n gi·∫£n ƒë·ªÉ sinh ra c√°c t·ª´ ti·∫øp theo.
    *   M√¥ h√¨nh GRU c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·∫ßu v√†o l√† m·ªôt chu·ªói v√† d·ª± ƒëo√°n t·ª´ ti·∫øp theo trong chu·ªói.

