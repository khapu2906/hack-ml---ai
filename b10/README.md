## üìö Bu·ªïi 10 ‚Äì Convolutional Neural Networks (CNN)

### üéØ M·ª•c ti√™u:

*   Hi·ªÉu v·ªÅ c√°c m·∫°ng n∆°-ron t√≠ch ch·∫≠p (CNN) v√† ·ª©ng d·ª•ng c·ªßa ch√∫ng trong nh·∫≠n d·∫°ng h√¨nh ·∫£nh.
*   N·∫Øm v·ªØng c√°c l·ªõp c∆° b·∫£n trong CNN, nh∆∞ Convolution, Pooling, v√† Fully Connected.
*   T√¨m hi·ªÉu c√°ch CNN ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c b√†i to√°n nh·∫≠n d·∫°ng ·∫£nh v√† ph√¢n lo·∫°i.

---

### üîç N·ªôi dung ch√≠nh:

#### 1. **Gi·ªõi thi·ªáu v·ªÅ CNN:**

*   **Convolutional Neural Networks** (CNN) l√† m·ªôt lo·∫°i m·∫°ng n∆°-ron ƒë·∫∑c bi·ªát hi·ªáu qu·∫£ trong vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu c√≥ d·∫°ng l∆∞·ªõi, nh∆∞ h√¨nh ·∫£nh (2D) ho·∫∑c video (3D).
*   CNN ƒë∆∞·ª£c s·ª≠ d·ª•ng ph·ªï bi·∫øn trong c√°c b√†i to√°n nh∆∞ nh·∫≠n d·∫°ng ƒë·ªëi t∆∞·ª£ng trong ·∫£nh, ph√¢n lo·∫°i ·∫£nh, v√† nh·∫≠n di·ªán khu√¥n m·∫∑t.
*   CNN ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng b·ªüi ba lo·∫°i l·ªõp ch√≠nh: Convolutional, Pooling v√† Fully Connected.

**Gi·∫£i th√≠ch:**

*   **Convolutional Neural Networks (CNN):** M·∫°ng n∆°-ron t√≠ch ch·∫≠p, hi·ªáu qu·∫£ trong x·ª≠ l√Ω d·ªØ li·ªáu c√≥ c·∫•u tr√∫c l∆∞·ªõi.
*   **·ª®ng d·ª•ng:** C√°c ·ª©ng d·ª•ng ph·ªï bi·∫øn c·ªßa CNN.
*   **C√°c l·ªõp ch√≠nh:** C√°c l·ªõp c∆° b·∫£n t·∫°o n√™n CNN.

#### 2. **L·ªõp Convolution:**

*   L·ªõp Convolution th·ª±c hi·ªán ph√©p nh√¢n ma tr·∫≠n gi·ªØa m·ªôt kernel (ho·∫∑c filter) v√† v√πng t∆∞∆°ng ·ª©ng trong ·∫£nh ƒë·∫ßu v√†o.
*   **Ch·ª©c nƒÉng**: Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng (features) nh∆∞ c√°c c·∫°nh, g√≥c, ho·∫∑c k·∫øt c·∫•u.
*   C√¥ng th·ª©c t√≠nh to√°n Convolution:

    $$
    \text{Output} = \sum_{i=1}^{k} (W_i * X_i)
    $$

    Trong ƒë√≥, $W_i$ l√† c√°c b·ªô l·ªçc v√† $X_i$ l√† c√°c ph·∫ßn t·ª≠ ·∫£nh t·∫°i c√°c v·ªã tr√≠ t∆∞∆°ng ·ª©ng.

**Gi·∫£i th√≠ch:**

*   **L·ªõp Convolution:** L·ªõp th·ª±c hi·ªán ph√©p t√≠ch ch·∫≠p gi·ªØa kernel v√† ·∫£nh ƒë·∫ßu v√†o.
*   **Ch·ª©c nƒÉng:** Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh.
*   **C√¥ng th·ª©c t√≠nh to√°n:** C√¥ng th·ª©c to√°n h·ªçc c·ªßa ph√©p t√≠ch ch·∫≠p.

#### 3. **L·ªõp Pooling:**

*   L·ªõp Pooling gi·∫£m ƒë·ªô ph√¢n gi·∫£i kh√¥ng gian c·ªßa h√¨nh ·∫£nh, gi√∫p gi·∫£m thi·ªÉu s·ªë l∆∞·ª£ng th√¥ng tin c·∫ßn x·ª≠ l√Ω.
*   Hai lo·∫°i ch√≠nh: **Max Pooling** (ch·ªçn gi√° tr·ªã l·ªõn nh·∫•t trong m·ªôt v√πng) v√† **Average Pooling** (t√≠nh gi√° tr·ªã trung b√¨nh trong m·ªôt v√πng).

**Gi·∫£i th√≠ch:**

*   **L·ªõp Pooling:** L·ªõp gi·∫£m k√≠ch th∆∞·ªõc c·ªßa ·∫£nh.
*   **Max Pooling:** Ch·ªçn gi√° tr·ªã l·ªõn nh·∫•t trong m·ªôt v√πng.
*   **Average Pooling:** T√≠nh gi√° tr·ªã trung b√¨nh trong m·ªôt v√πng.

#### 4. **L·ªõp Fully Connected (FC):**

*   Sau khi c√°c ƒë·∫∑c tr∆∞ng ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t qua c√°c l·ªõp Convolution v√† Pooling, d·ªØ li·ªáu ƒë∆∞·ª£c chuy·ªÉn qua c√°c l·ªõp Fully Connected ƒë·ªÉ th·ª±c hi·ªán ph√¢n lo·∫°i.
*   M·ªói neuron trong l·ªõp FC li√™n k·∫øt v·ªõi t·∫•t c·∫£ c√°c neuron c·ªßa l·ªõp tr∆∞·ªõc ƒë√≥.

**Gi·∫£i th√≠ch:**

*   **L·ªõp Fully Connected:** L·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß, s·ª≠ d·ª•ng c√°c ƒë·∫∑c tr∆∞ng ƒë√£ tr√≠ch xu·∫•t ƒë·ªÉ ph√¢n lo·∫°i.

#### 5. **K·ªπ thu·∫≠t Data Augmentation:**

*   ƒê·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa CNN, k·ªπ thu·∫≠t **Data Augmentation** c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o ra nhi·ªÅu m·∫´u d·ªØ li·ªáu kh√°c nhau t·ª´ c√πng m·ªôt h√¨nh ·∫£nh g·ªëc (v√≠ d·ª•: xoay, thay ƒë·ªïi t·ª∑ l·ªá, c·∫Øt gh√©p).

**Gi·∫£i th√≠ch:**

*   **Data Augmentation:** K·ªπ thu·∫≠t tƒÉng c∆∞·ªùng d·ªØ li·ªáu b·∫±ng c√°ch t·∫°o ra c√°c bi·∫øn th·ªÉ c·ªßa h√¨nh ·∫£nh g·ªëc.

#### 6. **·ª®ng d·ª•ng c·ªßa CNN:**

*   **Nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng trong ·∫£nh**: Ph√°t hi·ªán v√† ph√¢n lo·∫°i c√°c ƒë·ªëi t∆∞·ª£ng trong ·∫£nh (v√≠ d·ª•: nh·∫≠n d·∫°ng khu√¥n m·∫∑t, ph√¢n lo·∫°i ·∫£nh ƒë·ªông v·∫≠t, v.v.).
*   **Ph√¢n lo·∫°i ·∫£nh**: CNN c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√¢n lo·∫°i c√°c ·∫£nh th√†nh c√°c nh√£n kh√°c nhau.
*   **Nh·∫≠n di·ªán vƒÉn b·∫£n**: CNN c√≥ th·ªÉ nh·∫≠n di·ªán vƒÉn b·∫£n trong ·∫£nh (OCR).

**Gi·∫£i th√≠ch:**

*   **Nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng trong ·∫£nh:** ·ª®ng d·ª•ng c·ªßa CNN trong vi·ªác ph√°t hi·ªán v√† ph√¢n lo·∫°i ƒë·ªëi t∆∞·ª£ng.
*   **Ph√¢n lo·∫°i ·∫£nh:** ·ª®ng d·ª•ng c·ªßa CNN trong vi·ªác ph√¢n lo·∫°i ·∫£nh.
*   **Nh·∫≠n di·ªán vƒÉn b·∫£n:** ·ª®ng d·ª•ng c·ªßa CNN trong vi·ªác nh·∫≠n di·ªán vƒÉn b·∫£n trong ·∫£nh.

---

### üß™ B√†i lab Bu·ªïi 10:

#### 1. **C√†i ƒë·∫∑t CNN ƒë∆°n gi·∫£n ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh CIFAR-10:**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# T·∫£i d·ªØ li·ªáu CIFAR-10
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

trainloader = DataLoader(trainset, batch_size=4, shuffle=True)
testloader = DataLoader(testset, batch_size=4, shuffle=False)

# ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)  # L·ªõp convolution ƒë·∫ßu ti√™n
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling
        self.conv2 = nn.Conv2d(6, 16, 5)  # L·ªõp convolution th·ª© hai
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layer
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)  # 10 l·ªõp output cho 10 lo·∫°i

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # K·∫øt h·ª£p Convolution v√† ReLU
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)  # Bi·∫øn ƒë·ªïi th√†nh vector 1D
        x = F.relu(self.fc1(x))  # Fully connected layer 1
        x = F.relu(self.fc2(x))  # Fully connected layer 2
        x = self.fc3(x)  # Output layer
        return x

# Kh·ªüi t·∫°o m√¥ h√¨nh v√† optimizer
model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Hu·∫•n luy·ªán m√¥ h√¨nh
for epoch in range(2):  # Loop qua nhi·ªÅu epoch
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 2000 == 1999:  # In m·ªói 2000 b∆∞·ªõc
            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

print('Finished Training')
```

**H∆∞·ªõng d·∫´n:**

*   C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt: `torch`, `torchvision`.
*   T·∫£i b·ªô d·ªØ li·ªáu CIFAR-10.
*   ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN.
*   Kh·ªüi t·∫°o m√¥ h√¨nh v√† optimizer.
*   Hu·∫•n luy·ªán m√¥ h√¨nh.

#### 2. **C·∫£i thi·ªán m√¥ h√¨nh v·ªõi Data Augmentation:**

```python
transform_augmented = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset_augmented = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_augmented)
trainloader_augmented = DataLoader(trainset_augmented, batch_size=4, shuffle=True)

# Hu·∫•n luy·ªán l·∫°i v·ªõi b·ªô d·ªØ li·ªáu augmented
```

**H∆∞·ªõng d·∫´n:**

*   T·∫°o m·ªôt transform ƒë·ªÉ th·ª±c hi·ªán data augmentation.
*   T·∫£i l·∫°i b·ªô d·ªØ li·ªáu CIFAR-10 v·ªõi transform m·ªõi.
*   Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh v·ªõi b·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c augmented.

---

#### 7. C√°c ki·∫øn tr√∫c CNN ph·ªï bi·∫øn:

*   **LeNet-5:**
    *   **Gi·∫£i th√≠ch:** M·ªôt trong nh·ªØng ki·∫øn tr√∫c CNN ƒë·∫ßu ti√™n, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ nh·∫≠n di·ªán ch·ªØ s·ªë vi·∫øt tay.
    *   **∆Øu ƒëi·ªÉm:** ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** Kh√¥ng hi·ªáu qu·∫£ v·ªõi c√°c b√†i to√°n ph·ª©c t·∫°p h∆°n.
*   **AlexNet:**
    *   **Gi·∫£i th√≠ch:** M·ªôt ki·∫øn tr√∫c CNN s√¢u h∆°n LeNet-5, ƒë√£ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ ·∫•n t∆∞·ª£ng trong cu·ªôc thi ImageNet.
    *   **∆Øu ƒëi·ªÉm:** S·ª≠ d·ª•ng ReLU activation function v√† dropout ƒë·ªÉ gi·∫£m overfitting.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** C·∫•u tr√∫c v·∫´n c√≤n kh√° ƒë∆°n gi·∫£n so v·ªõi c√°c ki·∫øn tr√∫c hi·ªán ƒë·∫°i.
*   **VGGNet:**
    *   **Gi·∫£i th√≠ch:** Ki·∫øn tr√∫c CNN s√¢u v·ªõi c√°c l·ªõp convolution nh·ªè (3x3).
    *   **∆Øu ƒëi·ªÉm:** C·∫•u tr√∫c ƒë·ªìng nh·∫•t, d·ªÖ m·ªü r·ªông.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** S·ªë l∆∞·ª£ng tham s·ªë l·ªõn, ƒë√≤i h·ªèi nhi·ªÅu t√†i nguy√™n t√≠nh to√°n.
*   **ResNet:**
    *   **Gi·∫£i th√≠ch:** Ki·∫øn tr√∫c CNN s·ª≠ d·ª•ng c√°c k·∫øt n·ªëi t·∫Øt (skip connections) ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ vanishing gradient trong c√°c m·∫°ng s√¢u.
    *   **∆Øu ƒëi·ªÉm:** Cho ph√©p hu·∫•n luy·ªán c√°c m·∫°ng r·∫•t s√¢u, ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët tr√™n nhi·ªÅu b√†i to√°n.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** C·∫•u tr√∫c ph·ª©c t·∫°p h∆°n so v·ªõi c√°c ki·∫øn tr√∫c tr∆∞·ªõc ƒë√≥.
*   **InceptionNet:**
    *   **Gi·∫£i th√≠ch:** Ki·∫øn tr√∫c CNN s·ª≠ d·ª•ng c√°c module Inception ƒë·ªÉ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ·ªü nhi·ªÅu k√≠ch th∆∞·ªõc kh√°c nhau.
    *   **∆Øu ƒëi·ªÉm:** Hi·ªáu qu·∫£ trong vi·ªác tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng ƒëa d·∫°ng.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** C·∫•u tr√∫c ph·ª©c t·∫°p.

---

### üìù B√†i t·∫≠p v·ªÅ nh√† Bu·ªïi 10:

1.  **C√†i ƒë·∫∑t CNN cho b·ªô d·ªØ li·ªáu MNIST**:

    *   T·∫°o m·ªôt m√¥ h√¨nh CNN cho b·ªô d·ªØ li·ªáu MNIST v√† th·ª±c hi·ªán ph√¢n lo·∫°i c√°c ch·ªØ s·ªë vi·∫øt tay. So s√°nh k·∫øt qu·∫£ v·ªõi m·ªôt m√¥ h√¨nh MLP (Multi-Layer Perceptron).
2.  **C·∫£i thi·ªán m√¥ h√¨nh CNN**:

    *   √Åp d·ª•ng k·ªπ thu·∫≠t Data Augmentation cho b·ªô d·ªØ li·ªáu CIFAR-10 v√† quan s√°t s·ª± thay ƒë·ªïi v·ªÅ ƒë·ªô ch√≠nh x√°c so v·ªõi m√¥ h√¨nh kh√¥ng c√≥ augmentation.
3.  **T√¨m hi·ªÉu c√°c ki·∫øn tr√∫c CNN kh√°c**:

    *   T√¨m hi·ªÉu v·ªÅ c√°c m√¥ h√¨nh CNN ph·ªï bi·∫øn nh∆∞ VGG, ResNet v√† Inception. So s√°nh c√°c ki·∫øn tr√∫c n√†y v·ªõi m√¥ h√¨nh CNN ƒë∆°n gi·∫£n.
