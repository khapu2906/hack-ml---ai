## ğŸ“š Buá»•i 9 â€“ Transformer & Attention

### ğŸ¯ Má»¥c tiÃªu:

*   Hiá»ƒu vá» kiáº¿n trÃºc Transformer, cÆ¡ cháº¿ Attention vÃ  á»©ng dá»¥ng cá»§a nÃ³ trong cÃ¡c mÃ´ hÃ¬nh NLP hiá»‡n Ä‘áº¡i nhÆ° BERT, GPT.
*   Náº¯m vá»¯ng cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a Attention, Self-Attention, Multi-Head Attention.
*   Cáº¥u trÃºc vÃ  cÃ¡ch huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer.

---

### ğŸ” Ná»™i dung chÃ­nh:

#### 1. **Giá»›i thiá»‡u vá» Transformer:**

*   **Transformer** lÃ  má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron máº¡nh máº½, Ä‘áº·c biá»‡t trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).
*   ÄÆ°á»£c giá»›i thiá»‡u trong bÃ i bÃ¡o "Attention Is All You Need" (Vaswani et al., 2017), Transformer thay tháº¿ cÃ¡c máº¡ng RNN/LSTM trong nhiá»u á»©ng dá»¥ng NLP vÃ  Ä‘Ã£ giÃºp Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t vÆ°á»£t trá»™i trong cÃ¡c tÃ¡c vá»¥ nhÆ° dá»‹ch mÃ¡y, phÃ¢n loáº¡i vÄƒn báº£n, vÃ  táº¡o vÄƒn báº£n.

**Giáº£i thÃ­ch:**

*   **Transformer:** Kiáº¿n trÃºc máº¡ng nÆ¡-ron dá»±a trÃªn cÆ¡ cháº¿ Attention.
*   **"Attention Is All You Need"**: TÃªn bÃ i bÃ¡o giá»›i thiá»‡u kiáº¿n trÃºc Transformer.

**ğŸ“š Tham kháº£o:**

*   [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

#### 2. **CÆ¡ cháº¿ Attention:**

*   **Attention** cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o nhá»¯ng pháº§n quan trá»ng trong dá»¯ liá»‡u Ä‘áº§u vÃ o, thay vÃ¬ xá»­ lÃ½ tuáº§n tá»± nhÆ° RNN.
*   **Self-Attention** lÃ  cÃ¡ch mÃ  má»—i tá»« trong cÃ¢u cÃ³ thá»ƒ "chÃº Ã½" Ä‘áº¿n cÃ¡c tá»« khÃ¡c trong cÃ¹ng má»™t cÃ¢u. NÃ³ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¢u mÃ  khÃ´ng cáº§n pháº£i xá»­ lÃ½ tuáº§n tá»±.
*   **Scaled Dot-Product Attention**:
    *   CÃ´ng thá»©c tÃ­nh Attention:

        $$
        \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
        $$
    *   $Q$: Query (truy váº¥n), $K$: Key (chÃ¬a khÃ³a), $V$: Value (giÃ¡ trá»‹), $d_k$: kÃ­ch thÆ°á»›c cá»§a vector khÃ³a.
*   **Multi-Head Attention**: Má»™t cáº£i tiáº¿n cá»§a Attention, cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o nhiá»u "khÃ­a cáº¡nh" cá»§a thÃ´ng tin Ä‘áº§u vÃ o cÃ¹ng má»™t lÃºc.

**Giáº£i thÃ­ch:**

*   **Attention:** CÆ¡ cháº¿ cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a dá»¯ liá»‡u.
*   **Self-Attention:** CÆ¡ cháº¿ Attention trong Ä‘Ã³ cÃ¡c tá»« trong cÃ¢u "chÃº Ã½" láº«n nhau.
*   **Scaled Dot-Product Attention:** CÃ´ng thá»©c tÃ­nh Attention.
*   **Multi-Head Attention:** CÆ¡ cháº¿ Attention vá»›i nhiá»u "Ä‘áº§u" Ä‘á»ƒ táº­p trung vÃ o nhiá»u khÃ­a cáº¡nh khÃ¡c nhau.

#### 3. **Encoder-Decoder trong Transformer:**

*   **Encoder**: Chuyá»ƒn Ä‘á»•i cÃ¢u Ä‘áº§u vÃ o thÃ nh má»™t chuá»—i cÃ¡c Ä‘áº¡i diá»‡n (representations).
*   **Decoder**: Sá»­ dá»¥ng chuá»—i Ä‘áº¡i diá»‡n nÃ y Ä‘á»ƒ táº¡o ra cÃ¢u Ä‘áº§u ra.
*   Má»—i Encoder vÃ  Decoder trong Transformer gá»“m nhiá»u lá»›p Attention vÃ  Feed Forward.

**Giáº£i thÃ­ch:**

*   **Encoder:** Pháº§n cá»§a Transformer Ä‘á»ƒ mÃ£ hÃ³a cÃ¢u Ä‘áº§u vÃ o.
*   **Decoder:** Pháº§n cá»§a Transformer Ä‘á»ƒ giáº£i mÃ£ vÃ  táº¡o ra cÃ¢u Ä‘áº§u ra.

#### 4. **BERT & GPT â€“ á»¨ng dá»¥ng cá»§a Transformer:**

*   **BERT (Bidirectional Encoder Representations from Transformers)**: MÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng pháº§n Encoder cá»§a Transformer vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh tá»« cáº£ hai phÃ­a cá»§a má»™t tá»«.
*   **GPT (Generative Pre-trained Transformer)**: MÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng pháº§n Decoder cá»§a Transformer, Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ sinh vÄƒn báº£n tá»« má»™t Ä‘oáº¡n vÄƒn báº£n Ä‘áº§u vÃ o.

**Giáº£i thÃ­ch:**

*   **BERT:** MÃ´ hÃ¬nh Transformer sá»­ dá»¥ng Encoder Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh.
*   **GPT:** MÃ´ hÃ¬nh Transformer sá»­ dá»¥ng Decoder Ä‘á»ƒ sinh vÄƒn báº£n.

**ğŸ“š Tham kháº£o:**

*   [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
*   [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

#### 5. **á»¨ng dá»¥ng cá»§a Transformer trong NLP:**

*   **Dá»‹ch mÃ¡y**: Chuyá»ƒn ngá»¯ giá»¯a cÃ¡c ngÃ´n ngá»¯ (viá»‡c sá»­ dá»¥ng cÆ¡ cháº¿ Attention giÃºp Transformer há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ tá»« xa trong cÃ¢u).
*   **Táº¡o vÄƒn báº£n**: CÃ¡c mÃ´ hÃ¬nh nhÆ° GPT sá»­ dá»¥ng Transformer Ä‘á»ƒ táº¡o vÄƒn báº£n má»›i dá»±a trÃªn má»™t Ä‘oáº¡n vÄƒn báº£n Ä‘áº§u vÃ o.
*   **PhÃ¢n loáº¡i vÄƒn báº£n**: DÃ¹ng Transformer Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c vÄƒn báº£n dá»±a trÃªn ngá»¯ cáº£nh cá»§a cÃ¡c tá»« trong cÃ¢u.

**Giáº£i thÃ­ch:**

*   **Dá»‹ch mÃ¡y:** á»¨ng dá»¥ng cá»§a Transformer trong viá»‡c dá»‹ch ngÃ´n ngá»¯.
*   **Táº¡o vÄƒn báº£n:** á»¨ng dá»¥ng cá»§a Transformer trong viá»‡c táº¡o vÄƒn báº£n má»›i.
*   **PhÃ¢n loáº¡i vÄƒn báº£n:** á»¨ng dá»¥ng cá»§a Transformer trong viá»‡c phÃ¢n loáº¡i vÄƒn báº£n.

---

### ğŸ§ª BÃ i lab Buá»•i 9:

#### 1. **CÃ i Ä‘áº·t Attention Ä‘Æ¡n giáº£n vá»›i PyTorch:**

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1)  # KÃ­ch thÆ°á»›c cá»§a key
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attention_weights = F.softmax(scores, dim=-1)
    return torch.matmul(attention_weights, V)

# VÃ­ dá»¥ Ä‘áº§u vÃ o
Q = torch.rand(1, 3, 4)  # (batch_size, seq_len, d_k)
K = torch.rand(1, 3, 4)
V = torch.rand(1, 3, 4)

output = scaled_dot_product_attention(Q, K, V)
print(output.shape)  # (1, 3, 4)
```

**HÆ°á»›ng dáº«n:**

*   CÃ i Ä‘áº·t hÃ m `scaled_dot_product_attention()` Ä‘á»ƒ tÃ­nh Attention.
*   Táº¡o cÃ¡c tensor Ä‘áº§u vÃ o `Q`, `K`, `V`.
*   Gá»i hÃ m `scaled_dot_product_attention()` vÃ  in ra kÃ­ch thÆ°á»›c cá»§a output.

#### 2. **Multi-Head Attention trong PyTorch:**

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.embed_size = embed_size
        self.head_dim = embed_size // num_heads

        self.values = nn.Linear(embed_size, self.head_dim)
        self.keys = nn.Linear(embed_size, self.head_dim)
        self.queries = nn.Linear(embed_size, self.head_dim)
        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)

    def forward(self, values, keys, query):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        values = values.reshape(N, value_len, self.num_heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)
        query = query.reshape(N, query_len, self.num_heads, self.head_dim)

        values = values.permute(2, 0, 1, 3)
        keys = keys.permute(2, 0, 1, 3)
        query = query.permute(2, 0, 1, 3)

        energy = torch.einsum("qnhd,knhd->qnk", [query, keys])  # (num_heads, N, query_len, key_len)
        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=-1)

        out = torch.einsum("qnk,knhd->qnhd", [attention, values]).permute(1, 2, 0, 3)
        out = out.reshape(N, query_len, self.num_heads * self.head_dim)

        out = self.fc_out(out)
        return out

# VÃ­ dá»¥ vá»›i dá»¯ liá»‡u
embedding_dim = 256
num_heads = 8
multihead_attention = MultiHeadAttention(embed_size=embedding_dim, num_heads=num_heads)

# Giáº£ sá»­ má»—i tá»« trong cÃ¢u cÃ³ vector embedding kÃ­ch thÆ°á»›c 256
sample_input = torch.rand(1, 5, embedding_dim)  # (batch_size, seq_len, embed_size)
output = multihead_attention(sample_input, sample_input, sample_input)
print(output.shape)  # (1, 5, 256)
```

**HÆ°á»›ng dáº«n:**

*   Táº¡o má»™t lá»›p `MultiHeadAttention` káº¿ thá»«a tá»« `nn.Module`.
*   Sá»­ dá»¥ng cÃ¡c lá»›p `nn.Linear` Ä‘á»ƒ táº¡o cÃ¡c ma tráº­n trá»ng sá»‘ cho `values`, `keys`, vÃ  `queries`.
*   Thá»±c hiá»‡n phÃ©p tÃ­nh Multi-Head Attention vÃ  in ra kÃ­ch thÆ°á»›c cá»§a output.

---

### ğŸ“ BÃ i táº­p vá» nhÃ  Buá»•i 9:

1.  **CÃ i Ä‘áº·t Self-Attention**:

    *   CÃ i Ä‘áº·t cÆ¡ cháº¿ Self-Attention theo cÃ´ng thá»©c Scaled Dot-Product Attention cho má»™t cÃ¢u vÄƒn báº£n máº«u.
    *   HÃ£y thá»­ nghiá»‡m vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau cá»§a vector Ä‘áº§u vÃ o.
2.  **Táº¡o mÃ´ hÃ¬nh Transformer Ä‘Æ¡n giáº£n**:

    *   XÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Transformer sá»­ dá»¥ng Encoder vÃ  Decoder Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t chuá»—i tiáº¿p theo tá»« má»™t chuá»—i Ä‘áº§u vÃ o. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng PyTorch hoáº·c TensorFlow.
3.  **TÃ¬m hiá»ƒu vá» BERT/GPT**:

    *   TÃ¬m hiá»ƒu cÃ¡ch BERT vÃ  GPT Ä‘Æ°á»£c huáº¥n luyá»‡n. LÃ m tháº¿ nÃ o chÃºng sá»­ dá»¥ng Transformer Ä‘á»ƒ hiá»ƒu vÃ  táº¡o vÄƒn báº£n?

